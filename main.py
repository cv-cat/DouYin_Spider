# coding=utf-8
import json
import os
import sys
from loguru import logger

from dy_apis.douyin_api import DouyinAPI
from utils.common_util import init
from utils.data_util import handle_work_info, download_work, save_to_xlsx


class Data_Spider():
    def __init__(self):
        self.douyin_apis = DouyinAPI()

    def spider_work(self, auth, work_url: str, proxies=None):
        """
        çˆ¬å–ä¸€ä¸ªä½œå“çš„ä¿¡æ¯
        :param auth : ç”¨æˆ·è®¤è¯ä¿¡æ¯
        :param work_url: ä½œå“é“¾æ¥
        :return:
        """
        res_json = self.douyin_apis.get_work_info(auth, work_url)
        data = res_json['aweme_detail']

        work_info = handle_work_info(data)
        logger.info(f'çˆ¬å–ä½œå“ä¿¡æ¯ {work_url}')
        return work_info

    def spider_some_work(self, auth, works: list, base_path: dict, save_choice: str, excel_name: str = '', proxies=None):
        """
        çˆ¬å–ä¸€äº›ä½œå“çš„ä¿¡æ¯
        :param auth: ç”¨æˆ·è®¤è¯ä¿¡æ¯
        :param works: ä½œå“é“¾æ¥åˆ—è¡¨
        :param base_path: ä¿å­˜è·¯å¾„
        :param save_choice: ä¿å­˜æ–¹å¼ all: ä¿å­˜æ‰€æœ‰çš„ä¿¡æ¯, media: ä¿å­˜è§†é¢‘å’Œå›¾ç‰‡ï¼ˆmedia-videoåªä¸‹è½½è§†é¢‘, media-imageåªä¸‹è½½å›¾ç‰‡ï¼Œmediaéƒ½ä¸‹è½½ï¼‰, excel: ä¿å­˜åˆ°excel
        :param excel_name: excelæ–‡ä»¶å
        :return:
        """
        if (save_choice == 'all' or save_choice == 'excel') and excel_name == '':
            raise ValueError('excel_name ä¸èƒ½ä¸ºç©º')
        work_list = []
        for work_url in works:
            work_info = self.spider_work(auth, work_url)
            work_list.append(work_info)
        for work_info in work_list:
            if save_choice == 'all' or 'media' in save_choice:
                download_work(auth, work_info, base_path['media'], save_choice)
        if save_choice == 'all' or save_choice == 'excel':
            file_path = os.path.abspath(os.path.join(base_path['excel'], f'{excel_name}.xlsx'))
            save_to_xlsx(work_list, file_path)


    def spider_user_all_work(self, auth, user_url: str, base_path: dict, save_choice: str, excel_name: str = '', proxies=None):
        """
        çˆ¬å–ä¸€ä¸ªç”¨æˆ·çš„æ‰€æœ‰ä½œå“
        :param auth: ç”¨æˆ·è®¤è¯ä¿¡æ¯
        :param user_url: ç”¨æˆ·é“¾æ¥
        :param base_path: ä¿å­˜è·¯å¾„
        :param save_choice: ä¿å­˜æ–¹å¼ all: ä¿å­˜æ‰€æœ‰çš„ä¿¡æ¯, media: ä¿å­˜è§†é¢‘å’Œå›¾ç‰‡ï¼ˆmedia-videoåªä¸‹è½½è§†é¢‘, media-imageåªä¸‹è½½å›¾ç‰‡ï¼Œmediaéƒ½ä¸‹è½½ï¼‰, excel: ä¿å­˜åˆ°excel
        :param excel_name: excelæ–‡ä»¶å
        :param proxies: ä»£ç†
        :return:
        """
        user_info = self.douyin_apis.get_user_info(auth, user_url)
        work_list = self.douyin_apis.get_user_all_work_info(auth, user_url)
        work_info_list = []
        logger.info(f'ç”¨æˆ· {user_url} ä½œå“æ•°é‡: {len(work_list)}')
        if save_choice == 'all' or save_choice == 'excel':
            excel_name = user_url.split('/')[-1].split('?')[0]

        for work_info in work_list:
            work_info['author'].update(user_info['user'])
            work_info = handle_work_info(work_info)
            work_info_list.append(work_info)
            logger.info(f'çˆ¬å–ä½œå“ä¿¡æ¯ {work_info["work_url"]}')
            if save_choice == 'all' or 'media' in save_choice:
                download_work(auth, work_info, base_path['media'], save_choice)
        if save_choice == 'all' or save_choice == 'excel':
            file_path = os.path.abspath(os.path.join(base_path['excel'], f'{excel_name}.xlsx'))
            save_to_xlsx(work_info_list, file_path)

    def spider_some_search_work(self, auth, query: str, require_num: int, base_path: dict, save_choice: str,  sort_type: str, publish_time: str, filter_duration="", search_range="", content_type="",   excel_name: str = '', proxies=None):
        """
            :param auth: DouyinAuth object.
            :param query: æœç´¢å…³é”®å­—.
            :param require_num: æœç´¢ç»“æœæ•°é‡.
            :param base_path: ä¿å­˜è·¯å¾„.
            :param save_choice: ä¿å­˜æ–¹å¼ all: ä¿å­˜æ‰€æœ‰çš„ä¿¡æ¯, media: ä¿å­˜è§†é¢‘å’Œå›¾ç‰‡ï¼ˆmedia-videoåªä¸‹è½½è§†é¢‘, media-imageåªä¸‹è½½å›¾ç‰‡ï¼Œmediaéƒ½ä¸‹è½½ï¼‰, excel: ä¿å­˜åˆ°excel
            :param sort_type: æ’åºæ–¹å¼ 0 ç»¼åˆæ’åº, 1 æœ€å¤šç‚¹èµ, 2 æœ€æ–°å‘å¸ƒ.
            :param publish_time: å‘å¸ƒæ—¶é—´ 0 ä¸é™, 1 ä¸€å¤©å†…, 7 ä¸€å‘¨å†…, 180 åŠå¹´å†….
            :param filter_duration: è§†é¢‘æ—¶é•¿ ç©ºå­—ç¬¦ä¸² ä¸é™, 0-1 ä¸€åˆ†é’Ÿå†…, 1-5 1-5åˆ†é’Ÿå†…, 5-10000 5åˆ†é’Ÿä»¥ä¸Š
            :param search_range: æœç´¢èŒƒå›´ 0 ä¸é™, 1 æœ€è¿‘çœ‹è¿‡, 2 è¿˜æœªçœ‹è¿‡, 3 å…³æ³¨çš„äºº
            :param content_type: å†…å®¹å½¢å¼ 0 ä¸é™, 1 è§†é¢‘, 2 å›¾æ–‡
            :param excel_name: excelæ–‡ä»¶å
        """
        work_info_list = []
        work_list = self.douyin_apis.search_some_general_work(auth, query, require_num, sort_type, publish_time, filter_duration, search_range, content_type)
        logger.info(f'æœç´¢å…³é”®è¯ {query} ä½œå“æ•°é‡: {len(work_list)}')
        if save_choice == 'all' or save_choice == 'excel':
            excel_name = query
        for work_info in work_list:
            logger.info(json.dumps(work_info))
            logger.info(f'çˆ¬å–ä½œå“ä¿¡æ¯ https://www.douyin.com/video/{work_info["aweme_info"]["aweme_id"]}')
            work_info = handle_work_info(work_info['aweme_info'])
            work_info_list.append(work_info)
            if save_choice == 'all' or 'media' in save_choice:
                download_work(auth, work_info, base_path['media'], save_choice)
        if save_choice == 'all' or save_choice == 'excel':
            file_path = os.path.abspath(os.path.join(base_path['excel'], f'{excel_name}.xlsx'))
            save_to_xlsx(work_info_list, file_path)

if __name__ == '__main__':
    """
        æ­¤æ–‡ä»¶ä¸ºçˆ¬è™«çš„å…¥å£æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œ
        dy_apis/douyin_apis.py ä¸ºçˆ¬è™«çš„apiæ–‡ä»¶ï¼ŒåŒ…å«æŠ–éŸ³çš„å…¨éƒ¨æ•°æ®æ¥å£ï¼Œå¯ä»¥ç»§ç»­å°è£…
        dy_live/server.py ä¸ºç›‘å¬æŠ–éŸ³ç›´æ’­çš„å…¥å£æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œ
        æ„Ÿè°¢starå’Œfollow
    """

    # æ‰“å° Docker è¿è¡Œæ–¹æ¡ˆ
    logger.info("=" * 80)
    logger.info("ğŸ³ Docker è¿è¡Œæ–¹æ¡ˆ:")
    logger.info("=" * 80)
    logger.info("1. è¿è¡Œ Docker å®¹å™¨:")
    logger.info("   docker run --rm -it \\")
    logger.info("     -v \"$((Resolve-Path .\\datas).Path):/app/datas\" \\")
    logger.info("     -e DY_COOKIES=\"$env:DY_COOKIES\" \\")
    logger.info("     -e DOUYIN_WORKS=\"$env:DOUYIN_WORKS\" \\")
    logger.info("     -e DOUYIN_USER_URL=\"$env:DOUYIN_USER_URL\" \\")
    logger.info("     douyin-spider:local")
    logger.info("=" * 80)
    logger.info("")

    auth, base_path = init()

    data_spider = Data_Spider()
    # save_choice: all: ä¿å­˜æ‰€æœ‰çš„ä¿¡æ¯, media: ä¿å­˜è§†é¢‘å’Œå›¾ç‰‡ï¼ˆmedia-videoåªä¸‹è½½è§†é¢‘, media-imageåªä¸‹è½½å›¾ç‰‡ï¼Œmediaéƒ½ä¸‹è½½ï¼‰, excel: ä¿å­˜åˆ°excel
    # save_choice ä¸º excel æˆ–è€… all æ—¶ï¼Œexcel_name ä¸èƒ½ä¸ºç©º


    # 1 çˆ¬å–åˆ—è¡¨çš„æ‰€æœ‰ä½œå“ä¿¡æ¯ ä½œå“é“¾æ¥ ä»ç¯å¢ƒå˜é‡è·å–
    works_env = os.getenv('DOUYIN_WORKS', '')
    works = []
    if works_env:
        works = [url.strip() for url in works_env.split(',') if url.strip()]
    
    # 2 çˆ¬å–ç”¨æˆ·çš„æ‰€æœ‰ä½œå“ä¿¡æ¯ ç”¨æˆ·é“¾æ¥ ä»ç¯å¢ƒå˜é‡è·å–
    user_url = os.getenv('DOUYIN_USER_URL', '')
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦ä¸ºç©º
    if not works and not user_url:
        logger.error("ç¯å¢ƒå˜é‡ DOUYIN_WORKS å’Œ DOUYIN_USER_URL éƒ½ä¸ºç©ºï¼Œè¯·è®¾ç½®è‡³å°‘ä¸€ä¸ªç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # å¦‚æœworksä¸ä¸ºç©ºï¼Œåˆ™çˆ¬å–ä½œå“åˆ—è¡¨
    if works:
        data_spider.spider_some_work(auth, works, base_path, 'all', 'test')
    
    # å¦‚æœuser_urlä¸ä¸ºç©ºï¼Œåˆ™çˆ¬å–ç”¨æˆ·æ‰€æœ‰ä½œå“
    if user_url:
        data_spider.spider_user_all_work(auth, user_url, base_path, 'all')

    # 3 æœç´¢æŒ‡å®šå…³é”®è¯çš„ä½œå“
    # query = "æ¦´è²"
    # require_num = 20  # æœç´¢çš„æ•°é‡
    # sort_type = '0'  # æ’åºæ–¹å¼ 0 ç»¼åˆæ’åº, 1 æœ€å¤šç‚¹èµ, 2 æœ€æ–°å‘å¸ƒ
    # publish_time = '0'  # å‘å¸ƒæ—¶é—´ 0 ä¸é™, 1 ä¸€å¤©å†…, 7 ä¸€å‘¨å†…, 180 åŠå¹´å†…
    # filter_duration = ""  # è§†é¢‘æ—¶é•¿ ç©ºå­—ç¬¦ä¸² ä¸é™, 0-1 ä¸€åˆ†é’Ÿå†…, 1-5 1-5åˆ†é’Ÿå†…, 5-10000 5åˆ†é’Ÿä»¥ä¸Š
    # search_range = "0"  # æœç´¢èŒƒå›´ 0 ä¸é™, 1 æœ€è¿‘çœ‹è¿‡, 2 è¿˜æœªçœ‹è¿‡, 3 å…³æ³¨çš„äºº
    # content_type = "0"  # å†…å®¹å½¢å¼ 0 ä¸é™, 1 è§†é¢‘, 2 å›¾æ–‡

    # data_spider.spider_some_search_work(auth, query, require_num, base_path, 'all', sort_type, publish_time, filter_duration, search_range, content_type)

